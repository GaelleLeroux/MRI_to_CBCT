{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc53bd-56f5-49cc-b95a-105f52cb4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from math import pi\n",
    "import SimpleITK as sitk\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import vtk\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "import pickle\n",
    "from monai.transforms import (RandSimulateLowResolution)\n",
    "import monai \n",
    "\n",
    "\n",
    "alpha_coeff_boundary_map = 0.1\n",
    "beta_coeff_scattering = 10  #100 approximates it closer\n",
    "TGC = 8\n",
    "CLAMP_VALS = True\n",
    "\n",
    "\n",
    "def gaussian_kernel(size: int, mean: float, std: float):\n",
    "    d1 = torch.distributions.Normal(mean, std)\n",
    "    d2 = torch.distributions.Normal(mean, std*3)\n",
    "    vals_x = d1.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
    "    vals_y = d2.log_prob(torch.arange(-size, size+1, dtype=torch.float32)).exp()\n",
    "\n",
    "    gauss_kernel = torch.einsum('i,j->ij', vals_x, vals_y)\n",
    "    \n",
    "    return gauss_kernel / torch.sum(gauss_kernel).reshape(1, 1)\n",
    "\n",
    "g_kernel = gaussian_kernel(3, 0., 0.5)\n",
    "g_kernel = torch.tensor(g_kernel[None, None, :, :])\n",
    "\n",
    "\n",
    "class UltrasoundRendering(torch.nn.Module):\n",
    "    def __init__(self, num_labels=340, grid_width=1024, grid_height=1024, center_x=512, center_y=-80, r1=160.0, r2=896.0, theta=np.pi / 4):\n",
    "        super(UltrasoundRendering, self).__init__()               \n",
    "        \n",
    "        # df = pd.read_csv(acoustic_params_fn)        \n",
    "        # accoustic_imped,attenuation,mu_0,mu_1,sigma_0\n",
    "        self.num_labels = num_labels\n",
    "        self.acoustic_impedance_dict = torch.nn.Parameter(torch.rand(self.num_labels))    # Z in MRayl\n",
    "        self.attenuation_dict =    torch.nn.Parameter(torch.rand(self.num_labels))   # alpha in dB cm^-1 at 1 MHz\n",
    "        self.mu_0_dict =           torch.nn.Parameter(torch.rand(self.num_labels)) # mu_0 - scattering_mu   mean brightness\n",
    "        self.mu_1_dict =           torch.nn.Parameter(torch.rand(self.num_labels)) # mu_1 - scattering density, Nr of scatterers/voxel\n",
    "        self.sigma_0_dict =        torch.nn.Parameter(torch.rand(self.num_labels)) # sigma_0 - scattering_sigma - brightness std\n",
    "        \n",
    "        self.grid = self.compute_grid(grid_width, grid_height, center_x, center_y, r1, r2, theta)\n",
    "        \n",
    "        self.inverse_grid = self.compute_grid_inverse(self.grid)\n",
    "                \n",
    "        \n",
    "    def init_params(df):\n",
    "        df = pd.read_csv(acoustic_params_fn)\n",
    "        \n",
    "        # accoustic_imped,attenuation,mu_0,mu_1,sigma_0\n",
    "        self.acoustic_impedance_dict = torch.nn.Parameter(torch.tensor(df['accoustic_imped']))    # Z in MRayl\n",
    "        self.attenuation_dict =    torch.nn.Parameter(torch.tensor(df['attenuation']))   # alpha in dB cm^-1 at 1 MHz\n",
    "        self.mu_0_dict =           torch.nn.Parameter(torch.tensor(df['mu_0'])) # mu_0 - scattering_mu   mean brightness\n",
    "        self.mu_1_dict =           torch.nn.Parameter(torch.tensor(df['mu_1'])) # mu_1 - scattering density, Nr of scatterers/voxel\n",
    "        self.sigma_0_dict =        torch.nn.Parameter(torch.tensor(df['sigma_0'])) # sigma_0 - scattering_sigma - brightness std\n",
    "\n",
    "\n",
    "    def rendering(self, shape, attenuation_medium_map, mu_0_map, mu_1_map, sigma_0_map, z_vals=None, refl_map=None, boundary_map=None):\n",
    "        \n",
    "        dists = torch.abs(z_vals[..., :-1, None] - z_vals[..., 1:, None])     # dists.shape=(W, H-1, 1)\n",
    "        dists = dists.squeeze(-1)                                             # dists.shape=(W, H-1)\n",
    "        dists = torch.cat([dists, dists[:, -1, None]], dim=-1)                # dists.shape=(W, H)\n",
    "\n",
    "        attenuation = torch.exp(-attenuation_medium_map * dists)\n",
    "        attenuation_total = torch.cumprod(attenuation, dim=3, dtype=torch.float32, out=None)\n",
    "\n",
    "        gain_coeffs = np.linspace(1, TGC, attenuation_total.shape[3])\n",
    "        gain_coeffs = np.tile(gain_coeffs, (attenuation_total.shape[2], 1))\n",
    "        gain_coeffs = torch.tensor(gain_coeffs)\n",
    "        attenuation_total = attenuation_total * gain_coeffs     # apply TGC\n",
    "\n",
    "        reflection_total = torch.cumprod(1. - refl_map * boundary_map, dim=3, dtype=torch.float32, out=None) \n",
    "        reflection_total = reflection_total.squeeze(-1) \n",
    "        reflection_total_plot = torch.log(reflection_total + torch.finfo(torch.float32).eps)\n",
    "\n",
    "        texture_noise = torch.randn(shape, dtype=torch.float32)\n",
    "        scattering_probability = torch.randn(shape, dtype=torch.float32)\n",
    "\n",
    "        scattering_zero = torch.zeros(shape, dtype=torch.float32)\n",
    "\n",
    "        z = mu_1_map - scattering_probability\n",
    "        sigmoid_map = torch.sigmoid(beta_coeff_scattering * z)\n",
    "\n",
    "        # approximating  Eq. (4) to be differentiable:\n",
    "        # where(scattering_probability <= mu_1_map, \n",
    "        #                     texture_noise * sigma_0_map + mu_0_map, \n",
    "        #                     scattering_zero)\n",
    "        # scatterers_map =  (sigmoid_map) * (texture_noise * sigma_0_map + mu_0_map) + (1 -sigmoid_map) * scattering_zero   # Eq. (6)\n",
    "        scatterers_map =  (sigmoid_map) * (texture_noise * sigma_0_map + mu_0_map)\n",
    "\n",
    "        psf_scatter_conv = torch.nn.functional.conv2d(input=scatterers_map, weight=g_kernel, stride=1, padding=\"same\")\n",
    "        # psf_scatter_conv = psf_scatter_conv.squeeze()\n",
    "\n",
    "        b = attenuation_total * psf_scatter_conv    # Eq. (3)\n",
    "\n",
    "        border_convolution = torch.nn.functional.conv2d(input=boundary_map, weight=g_kernel, stride=1, padding=\"same\")\n",
    "        # border_convolution = border_convolution.squeeze()\n",
    "\n",
    "        r = attenuation_total * reflection_total * refl_map * border_convolution # Eq. (2)\n",
    "        \n",
    "        intensity_map = b + r   # Eq. (1)\n",
    "        # intensity_map = intensity_map.squeeze() \n",
    "        intensity_map = torch.clamp(intensity_map, 0, 1)\n",
    "\n",
    "        return intensity_map, attenuation_total, reflection_total_plot, scatterers_map, scattering_probability, border_convolution, texture_noise, b, r\n",
    "    \n",
    "    def render_rays(self, W, H):\n",
    "        N_rays = W \n",
    "        t_vals = torch.linspace(0., 1., H)\n",
    "        z_vals = t_vals.unsqueeze(0).expand(N_rays , -1) * 4 \n",
    "\n",
    "        return z_vals\n",
    "\n",
    "    def compute_grid(self, w, h, center_x, center_y, r1, r2, theta):\n",
    "        # Convert inputs to tensors\n",
    "        angles = torch.linspace(-theta, theta, w)  # Angles from -theta to theta\n",
    "        radii = torch.linspace(r1, r2, h)  # Linear space of radii\n",
    "\n",
    "        # Calculate sin and cos for all angles (broadcasting)\n",
    "        sin_angles = torch.sin(angles)\n",
    "        cos_angles = torch.cos(angles)\n",
    "\n",
    "        # Initialize the grid for intersection points\n",
    "        # Shape of grid: (h, w, 2) where 2 represents (x, y) coordinates\n",
    "        grid = torch.zeros(h, w, 2)\n",
    "\n",
    "        # Calculate intersections for each radius and angle\n",
    "        for i, radius in enumerate(radii):\n",
    "            x = (center_x + radius * sin_angles) # x coordinates for all angles at this radius\n",
    "            y = (center_y + radius * cos_angles) # y coordinates for all angles at this radius\n",
    "\n",
    "            grid[i] = torch.stack((y, x), dim=1)  # Update grid with coordinates\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def compute_grid_inverse(self, grid):\n",
    "        h, w, _ = grid.shape  # grid dimensions\n",
    "        inverse_grid = torch.zeros(h, w, 2)  # Initialize inverse grid\n",
    "\n",
    "        # Iterate through each point in the grid\n",
    "        for j in range(h):\n",
    "            for i in range(w):\n",
    "                # Extract the polar coordinates (represented in the grid)\n",
    "                xi, yi = torch.round(grid[j, i]).to(torch.long)\n",
    "                # xi = torch.int(x)\n",
    "\n",
    "\n",
    "                # Convert back to Cartesian coordinates\n",
    "                # x = r * torch.cos(theta) + center_x\n",
    "                # y = r * torch.sin(theta) + center_y\n",
    "\n",
    "                # Place the Cartesian coordinates in the inverse grid\n",
    "                if 0 <= xi and xi < w and 0 <= yi and yi < h:\n",
    "                    inverse_grid[yi, xi] = torch.tensor([i, j])\n",
    "\n",
    "        return inverse_grid\n",
    "    \n",
    "    def grid_transform(self, x, grid, interpolation_mode='nearest'):\n",
    "        \n",
    "        w, h, _ = grid.shape\n",
    "        \n",
    "        repeats = [1,]*len(x.shape)\n",
    "        repeats[0] = x.shape[0]\n",
    "        grid_f = grid / torch.tensor([w, h]) * 2.0 - 1.0\n",
    "        grid_f = grid_f.repeat(repeats)\n",
    "        \n",
    "        return F.grid_sample(x.float(), grid_f, mode=interpolation_mode)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #init tissue maps\n",
    "        #generate maps from the dictionary and the input label map\n",
    "        x = torch.rot90(x, k=1, dims=[2, 3])\n",
    "        \n",
    "        x = self.grid_transform(x, self.grid)\n",
    "        \n",
    "        x = x.to(torch.long)\n",
    "        \n",
    "        acoustic_imped_map = self.acoustic_impedance_dict[x]\n",
    "        attenuation_medium_map = self.attenuation_dict[x]\n",
    "        mu_0_map = self.mu_0_dict[x]\n",
    "        mu_1_map = self.mu_1_dict[x]\n",
    "        sigma_0_map = self.sigma_0_dict[x]\n",
    "\n",
    "        \n",
    "        #Comput the difference along dimension 2\n",
    "        diff_arr = torch.diff(acoustic_imped_map, dim=2)                \n",
    "        # The pad tuple is (padding_left,padding_right, padding_top,padding_bottom)\n",
    "        # The array is padded at the top\n",
    "        diff_arr = F.pad(diff_arr, (0,0,1,0))\n",
    "\n",
    "        #Compute the boundary map using the diff_array\n",
    "        boundary_map =  -torch.exp(-(diff_arr**2)/alpha_coeff_boundary_map) + 1\n",
    "        \n",
    "        #Roll/shift the elements along dimension 2 and set the last element to 0\n",
    "        shifted_arr = torch.roll(acoustic_imped_map, -1, dims=2)\n",
    "        shifted_arr[-1:] = 0\n",
    "\n",
    "        # This computes the sum/accumulation along the direction and set elements that are 0 to 1. Compute the division\n",
    "        sum_arr = acoustic_imped_map + shifted_arr\n",
    "        sum_arr[sum_arr == 0] = 1\n",
    "        div = diff_arr / sum_arr\n",
    "        # Compute the reflection from the elements\n",
    "        refl_map = div ** 2\n",
    "        refl_map = torch.sigmoid(refl_map)      # 1 / (1 + (-refl_map).exp())\n",
    "\n",
    "        z_vals = self.render_rays(x.shape[2], x.shape[3])\n",
    "\n",
    "        if CLAMP_VALS:\n",
    "            attenuation_medium_map = torch.clamp(attenuation_medium_map, 0, 10)\n",
    "            acoustic_imped_map = torch.clamp(acoustic_imped_map, 0, 10)\n",
    "            sigma_0_map = torch.clamp(sigma_0_map, 0, 1)\n",
    "            mu_1_map = torch.clamp(mu_1_map, 0, 1)\n",
    "            mu_0_map = torch.clamp(mu_0_map, 0, 1)\n",
    "\n",
    "        ret_list = self.rendering(x.shape, attenuation_medium_map, mu_0_map, mu_1_map, sigma_0_map, z_vals=z_vals, refl_map=refl_map, boundary_map=boundary_map)\n",
    "\n",
    "        intensity_map  = ret_list[0]\n",
    "       \n",
    "        # return intensity_map\n",
    "        intensity_map_t = self.grid_transform(intensity_map, self.inverse_grid)\n",
    "        \n",
    "        intensity_map = torch.rot90(intensity_map, k=3, dims=[2, 3])\n",
    "        intensity_map_t = torch.rot90(intensity_map_t, k=3, dims=[2, 3])\n",
    "\n",
    "        return intensity_map_t, attenuation_medium_map, mu_0_map, mu_1_map, sigma_0_map, acoustic_imped_map, boundary_map, shifted_arr, intensity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e6e40-083b-4e9e-a11f-d2e8064e1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_params_df = pd.read_csv('/mnt/raid/C1_ML_Analysis/source/guibruss/LOTUS/lotus/acoustic_params.csv')\n",
    "us_render = UltrasoundRendering(num_labels=len(acoustic_params_df.index))\n",
    "fake_us = us_render(torch.randint(low=0, high=10, size=(2, 1, 256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1204897-432b-4181-a814-0a9702c7ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/FAM-202-1960-2_mesh_sampling/AC/40.nrrd\"\n",
    "img = sitk.ReadImage(fn)\n",
    "img1_np = sitk.GetArrayFromImage(img)\n",
    "px.imshow(img1_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c1cc5-bde1-4d8f-a209-0f1c798477be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/FAM-202-1960-2_mesh_sampling/AC/98.nrrd\"\n",
    "img = sitk.ReadImage(fn)\n",
    "img2_np = sitk.GetArrayFromImage(img)\n",
    "px.imshow(img2_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbeb3f-f3cd-4ac0-9461-97bc70aeb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = np.stack([img1_np, img2_np]).astype(np.long)\n",
    "img_np.shape\n",
    "t = torch.tensor(img_np).to(torch.long)\n",
    "fake_us = us_render(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b7174-b50f-4357-9752-57dd6bb2fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(fake_us[0][0].detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858088da-9f40-44d7-bf84-b7e4db7f2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(fake_us[0][1].detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5d8bd-aeb7-4c37-a30b-d41e465d6164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ResnetGenerator(nn.Module):\n",
    "#     \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "\n",
    "#     We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
    "#         \"\"\"Construct a Resnet-based generator\n",
    "\n",
    "#         Parameters:\n",
    "#             input_nc (int)      -- the number of channels in input images\n",
    "#             output_nc (int)     -- the number of channels in output images\n",
    "#             ngf (int)           -- the number of filters in the last conv layer\n",
    "#             norm_layer          -- normalization layer\n",
    "#             use_dropout (bool)  -- if use dropout layers\n",
    "#             n_blocks (int)      -- the number of ResNet blocks\n",
    "#             padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "#         \"\"\"\n",
    "#         assert(n_blocks >= 0)\n",
    "#         super(ResnetGenerator, self).__init__()\n",
    "#         if type(norm_layer) == functools.partial:\n",
    "#             use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "#         else:\n",
    "#             use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "#         model = [nn.ReflectionPad2d(3),\n",
    "#                  nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "#                  norm_layer(ngf),\n",
    "#                  nn.ReLU(True)]\n",
    "\n",
    "#         n_downsampling = 2\n",
    "#         for i in range(n_downsampling):  # add downsampling layers\n",
    "#             mult = 2 ** i\n",
    "#             model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "#                       norm_layer(ngf * mult * 2),\n",
    "#                       nn.ReLU(True)]\n",
    "\n",
    "#         mult = 2 ** n_downsampling\n",
    "#         for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "#             model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "#         for i in range(n_downsampling):  # add upsampling layers\n",
    "#             mult = 2 ** (n_downsampling - i)\n",
    "#             model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "#                                          kernel_size=3, stride=2,\n",
    "#                                          padding=1, output_padding=1,\n",
    "#                                          bias=use_bias),\n",
    "#                       norm_layer(int(ngf * mult / 2)),\n",
    "#                       nn.ReLU(True)]\n",
    "#         model += [nn.ReflectionPad2d(3)]\n",
    "#         model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "#         model += [nn.Tanh()]\n",
    "\n",
    "#         self.model = nn.Sequential(*model)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         \"\"\"Standard forward\"\"\"\n",
    "#         return self.model(input)\n",
    "    \n",
    "# class ResnetBlock(nn.Module):\n",
    "#     \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "#     def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "#         \"\"\"Initialize the Resnet block\n",
    "\n",
    "#         A resnet block is a conv block with skip connections\n",
    "#         We construct a conv block with build_conv_block function,\n",
    "#         and implement skip connections in <forward> function.\n",
    "#         Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "#         \"\"\"\n",
    "#         super(ResnetBlock, self).__init__()\n",
    "#         self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "#     def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "#         \"\"\"Construct a convolutional block.\n",
    "\n",
    "#         Parameters:\n",
    "#             dim (int)           -- the number of channels in the conv layer.\n",
    "#             padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "#             norm_layer          -- normalization layer\n",
    "#             use_dropout (bool)  -- if use dropout layers.\n",
    "#             use_bias (bool)     -- if the conv layer uses bias or not\n",
    "\n",
    "#         Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "#         \"\"\"\n",
    "#         conv_block = []\n",
    "#         p = 0\n",
    "#         if padding_type == 'reflect':\n",
    "#             conv_block += [nn.ReflectionPad2d(1)]\n",
    "#         elif padding_type == 'replicate':\n",
    "#             conv_block += [nn.ReplicationPad2d(1)]\n",
    "#         elif padding_type == 'zero':\n",
    "#             p = 1\n",
    "#         else:\n",
    "#             raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "#         conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "#         if use_dropout:\n",
    "#             conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "#         p = 0\n",
    "#         if padding_type == 'reflect':\n",
    "#             conv_block += [nn.ReflectionPad2d(1)]\n",
    "#         elif padding_type == 'replicate':\n",
    "#             conv_block += [nn.ReplicationPad2d(1)]\n",
    "#         elif padding_type == 'zero':\n",
    "#             p = 1\n",
    "#         else:\n",
    "#             raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "#         conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "#         return nn.Sequential(*conv_block)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"Forward function (with skip connections)\"\"\"\n",
    "#         out = x + self.conv_block(x)  # add skip connections\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb7eaa-08bf-48f6-a042-7ea01a4f930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = vtk.vtkSTLReader()\n",
    "reader.SetFileName('/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/FAM-202-1960-2_mesh/gestational/gestational_sac.stl')\n",
    "reader.Update()\n",
    "surf = reader.GetOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006aa903-4e06-482a-a9ce-1b6e2ae548e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = sitk.ReadImage('/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/FAM-202-1960-2_mesh.nrrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96979a61-f7d2-4779-aafa-ac09333e8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe_params_df = pd.read_csv(\"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/CSV_files/FAM-202-1960-2_mesh_probe_params.csv\")\n",
    "probe_params_df = pd.read_csv(\"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/CSV_files/FAM-202-1960-2_mesh_probe_params_BPD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64955456-b419-4e23-be14-bcc3e1828568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VolumeSlicingDataset(Dataset):\n",
    "    def __init__(self, volume, surf, num_samples, ref_size=(256, 256, 1), ref_spacing=(0.0004119873046875, 0.0004119873046875, 0.0010500028729438782), transform=None, interpolator=sitk.sitkNearestNeighbor):\n",
    "                \n",
    "        self.volume = volume\n",
    "        self.surf = self.ComputeNormals(surf)\n",
    "        self.surf_points =  vtk_to_numpy(self.surf.GetPoints().GetData())\n",
    "        self.surf_normals = vtk_to_numpy(self.surf.GetPointData().GetArray(\"Normals\"))\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "        self.ref_size = ref_size\n",
    "        self.ref_spacing = ref_spacing\n",
    "        self.interpolator = interpolator\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self.sample_image()\n",
    "        img_np = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(img_np)\n",
    "        \n",
    "        return img_np\n",
    "    \n",
    "    def ComputeNormals(self, surf):\n",
    "        normals = vtk.vtkPolyDataNormals()\n",
    "        normals.SetInputData(surf);\n",
    "        normals.ComputeCellNormalsOn();\n",
    "        normals.ComputePointNormalsOn();\n",
    "        normals.SplittingOff();\n",
    "        normals.Update()\n",
    "\n",
    "        return normals.GetOutput()    \n",
    "\n",
    "    def align_matrix(self, N, A=np.array([-1.0, 0.0, 0.0])):\n",
    "        # Calculate the axis of rotation (cross product of A and N)\n",
    "        axis = np.cross(A, N)\n",
    "        axis_norm = np.linalg.norm(axis)\n",
    "        if axis_norm == 0:\n",
    "            # No rotation needed if vectors are parallel\n",
    "            return np.identity(3)\n",
    "        axis = axis / axis_norm\n",
    "\n",
    "        # Calculate the angle of rotation using the dot product\n",
    "        angle = np.arccos(np.clip(np.dot(A, N) / (np.linalg.norm(A) * np.linalg.norm(N)), -1.0, 1.0))\n",
    "       \n",
    "        return self.rotation_matrix(axis, angle)\n",
    "    \n",
    "    def rotation_matrix(self, axis, angle):\n",
    "        \"\"\"\n",
    "        Rotate the matrix 'M' around the 'axis' by 'angle' degrees.\n",
    "\n",
    "        Parameters:\n",
    "        v (np.array): The vector to be rotated.\n",
    "        axis (np.array): The axis of rotation (should be a normalized vector).\n",
    "        angle (float): The angle of rotation in degrees.\n",
    "\n",
    "        Returns:\n",
    "        np.array: The rotated vector.\n",
    "        \"\"\"\n",
    "        # Convert the angle from degrees to radians\n",
    "        angle_rad = np.radians(angle)\n",
    "\n",
    "        # Rodrigues' rotation formula components\n",
    "        axis = axis / np.linalg.norm(axis)  # Ensure the axis is a unit vector\n",
    "        K = np.array([[0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0]])\n",
    "        R = np.eye(3) + np.sin(angle_rad) * K + (1 - np.cos(angle_rad)) * np.dot(K, K)\n",
    "\n",
    "        # Apply the rotation\n",
    "        return R\n",
    "    \n",
    "    def create_4x4_matrix(self, R, P):\n",
    "        \"\"\"\n",
    "        Create a transformation matrix in homogeneous coordinates from a rotation matrix and a translation vector.\n",
    "\n",
    "        Parameters:\n",
    "        R (np.array): The 3x3 rotation matrix.\n",
    "        P (np.array): The translation vector.\n",
    "\n",
    "        Returns:\n",
    "        np.array: The 4x4 transformation matrix in homogeneous coordinates.\n",
    "        \"\"\"\n",
    "        T = np.zeros((4, 4))\n",
    "        T[:3, :3] = R\n",
    "        T[:3, 3] = P\n",
    "        T[3, 3] = 1\n",
    "        return T\n",
    "\n",
    "    def sample_image(self):\n",
    "        \n",
    "        random_i = np.random.randint(low=0, high=self.surf_points.shape[0])\n",
    "        \n",
    "        surf_point = self.surf_points[random_i]\n",
    "        # surf_point = np.array([-0.006, 0.033, 0.21])\n",
    "        surf_normal = self.surf_normals[random_i]\n",
    "        # surf_normal = np.array([0, 0, 1.0])       \n",
    "        \n",
    "        #This matrix aligns the vector (0, 0, 1) to the surface normal\n",
    "        rotation_matrix = self.align_matrix(surf_normal)\n",
    "        matrix_world = self.create_4x4_matrix(rotation_matrix, surf_point)\n",
    "        direction = matrix_world[0:3, 0:3]\n",
    "        \n",
    "        delta_origin = np.array([0, -self.ref_size[1]*self.ref_spacing[1]/2.0, -self.ref_size[2]*self.ref_spacing[2]/2.0, 1.0])\n",
    "        delta_origin = np.dot(matrix_world, delta_origin)\n",
    "        \n",
    "\n",
    "        ref = sitk.Image(int(self.ref_size[0]), int(self.ref_size[1]), int(self.ref_size[2]), sitk.sitkFloat32)\n",
    "        ref.SetOrigin(delta_origin[0:3].astype(np.double))\n",
    "        ref.SetSpacing(self.ref_spacing)\n",
    "        ref.SetDirection(direction.flatten().astype(np.double))\n",
    "\n",
    "        resampler = sitk.ResampleImageFilter()\n",
    "        if self.interpolator:\n",
    "            resampler.SetInterpolator(self.interpolator)\n",
    "        resampler.SetReferenceImage(ref)\n",
    "\n",
    "        return resampler.Execute(self.volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1efd7-7c03-4998-a653-bb0f9f47944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeSlicingProbeParamsDataset(Dataset):\n",
    "    def __init__(self, df, volume, probe_params_column_name=\"probe_params_fn\", mount_point=\"./\", transform=None, interpolator=sitk.sitkNearestNeighbor):\n",
    "                \n",
    "        self.df = df\n",
    "        self.volume = volume        \n",
    "        self.probe_params_column_name = probe_params_column_name\n",
    "        self.mount_point = mount_point\n",
    "        self.transform = transform        \n",
    "        self.interpolator = interpolator\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        probe_params_fn = self.df.iloc[idx][self.probe_params_column_name]        \n",
    "        probe_params = self.read_probe_params(probe_params_fn)\n",
    "\n",
    "        img = self.sample_image(probe_params)\n",
    "        img_np = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(img_np)\n",
    "        \n",
    "        return img_np\n",
    "    \n",
    "    def read_probe_params(self, probe_params_fn):\n",
    "        return pickle.load(open(os.path.join(self.mount_point, probe_params_fn), 'rb'))\n",
    "    \n",
    "    def sample_image(self, probe_params, interpolator=sitk.sitkNearestNeighbor, identity_direction=True):\n",
    "        \n",
    "        probe_direction = probe_params['probe_direction']\n",
    "        ref_size = probe_params['ref_size']\n",
    "        ref_origin = probe_params['ref_origin']\n",
    "        ref_spacing = probe_params['ref_spacing']\n",
    "\n",
    "        ref = sitk.Image(int(ref_size[0]), int(ref_size[1]), int(ref_size[2]), sitk.sitkFloat32)\n",
    "        ref.SetOrigin(ref_origin)\n",
    "        ref.SetSpacing(ref_spacing)\n",
    "        ref.SetDirection(probe_direction.flatten().tolist())\n",
    "\n",
    "        resampler = sitk.ResampleImageFilter()\n",
    "        if interpolator:\n",
    "            resampler.SetInterpolator(interpolator)\n",
    "        resampler.SetReferenceImage(ref)\n",
    "        \n",
    "        sample = resampler.Execute(self.volume)\n",
    "        if identity_direction:\n",
    "            sample_np = sitk.GetArrayFromImage(sample).squeeze()\n",
    "            sample_np = np.flip(np.rot90(sample_np, k=1, axes=(0, 1)), axis=0)\n",
    "            sample = sitk.GetImageFromArray(sample_np)\n",
    "            sample.SetSpacing(ref_spacing)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28931eb7-15c4-422b-99be-b1bdc5c1bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = VolumeSlicingProbeParamsDataset(probe_params_df, volume, mount_point=\"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32152d0-9fef-4075-9770-025097ce8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_np = ds[np.random.randint(low=0, high=len(probe_params_df.index))]\n",
    "px.imshow(sample_np.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec3bec-a35d-4e9a-857d-527e2f6dc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.empty(3, 5)\n",
    "nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb48425-c92a-45eb-9886-ffb1a73c20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img3_np = sitk.GetArrayFromImage(sitk.ReadImage(\"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/FAM-202-1960-2_mesh_sampling/BPD/20.nrrd\"))\n",
    "# img3_np.shape\n",
    "px.imshow(img3_np.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df7348-8487-466b-82f1-68bfd1f68c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(img3_np.astype(int)).to(torch.long)\n",
    "fake_us = us_render(t[None])\n",
    "px.imshow(fake_us[8][0].detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c237d5b-21e5-4150-98c0-5ffa7f69ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(fake_us[0][0].detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e4b90-7e83-49db-a2b8-ff59adc716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img4_np = sitk.GetArrayFromImage(sitk.ReadImage(\"/mnt/raid/C1_ML_Analysis/source/blender/simulated_data_export/FAM-202-1960-2_20211019_033119_split_frames/BPD/96.nrrd\"))\n",
    "# img3_np.shape\n",
    "px.imshow(img4_np.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0d37d-9e7a-4fd0-ac2e-731dddc7bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres = RandSimulateLowResolution(prob=1.0, zoom_range=(0.15, 0.3))\n",
    "img4_t = torch.tensor(img4_np).permute([2, 0, 1]).unsqueeze(dim=0)\n",
    "img4_t_lowres = lowres(img4_t)\n",
    "px.imshow(img4_t_lowres[0].permute(1, 2, 0).numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039be06-bb45-44fa-a501-8db173d4c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.blocks import (\n",
    "    ResidualUnit,\n",
    "    MLPBlock\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581f8ec-7745-4e20-84f3-e3a2a274e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = MLPBlock(256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a868270-bfdd-459b-a1ae-01b35c5a6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f(torch.rand(10, 1, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d776a9-0039-45a8-b19c-0d4eda8dea20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
